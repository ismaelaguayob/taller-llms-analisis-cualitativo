---
title: "Taller"
subtitle: "Gemini para análisis cualitativo en R: Un flujo de trabajo colaborativo y escalable"
author: "Ismael Aguayo y Exequiel Trujillo"
date: today
lang: en
fontsize: 14pt
format:
  html:
    code-fold: true
    toc: true
    toc-location: left
    toc-depth: 2
    toc-expand: 1
    toc-title: Contenidos
    number-sections: true
    theme: socialtec.scss
    code-link: true
    title-block-banner: true
bibliography: references.bib
csl: apa.csl
link-citations: true
---

## Antecedentes

### La revolución de la Inteligencia Artificial

-   *Attention is all you need* [@vaswani2017]: Artículo que crea la arquitectura transformer, que es la base de las IA actuales debido a su escalabilidad y eficiencia

-   Los grandes modelos de lenguaje (LLM por sus siglas en inglés) son modelos de Inteligencia Artificial entrenados en grandes volúmenes de datos, usualmente bajo la arquitectura de transformer, con la capacidad de comprender y generar texto para distintas tareas (traducción, programación, redacción, razonamiento, etc.)

-   30 de noviembre 2022: lanzamiento de ChatGPT. 100 millones de usuarios en un año.

-   Esto provocó un desarrollo acelerado de la IA en los últimos años:

    -   Áuge en la competencia y modelos open source: Claude, Gemini, Grok, Kimi-k2, Gemma, Gpt-Oss
    -   Múltiples aplicaciones: Vibe coding (Cursor, Windsurf), Ciencia (AlphaFold), Imágenes (MidJourney, NanoBanana), video (Veo3, Sora)
    -   Desarrollo de infraestructura clave: construcción de centros de datos y supercomputadores
    -   Avances en otras áreas como la robótica, la medicina, la neurociencia, etc.

### Herramientas de consumidor vs. herramientas de desarrollador

-   Dentro del mercado de la IA, existen distintos productos para distintas necesidades

-   Los servicios para los consumidores son los que comúnmente uno utiliza, los chatbot accesibles a través de las páginas web o aplicaciones de celular de ChatGpt o Gemini. Usualmente, tienen planes gratuitos y de pago.

![](images/chatgpt.jpg){fig-align="center"}

-   Por otro lado, están las plataformas para desarrolladores, comúnmente ofrecidas por las empresas a través de una Interfaz de Programación de Aplicaciones (API por sus siglas en inglés). A diferencia de los chatbots, las APIs cobran por cada llamada o mensaje enviado, y están diseñadas para poder desarrollar usos específicos de los LLM.

-   En este taller utilizaremos este segundo enfoque, para aprovechar los beneficios que puede otorgar la Inteligencia Artificial, pero manteniendo el control a la hora de procesar los datos y tomar las decisiones importantes en el análisis.

### LLMs para análisis cualitativo

-   Anteriormente, la forma de utilizar *text-as-data* más común era a través de modelos más básicos, con una menor capacidad de comprender el contexto amplio. Por ejemplo, para identificar tipos de consumidores en un dataset de comentarios, se podía ejecutar un LDA para modelar los tópicos subyacentes.

-   Desde la arquitectura transformers, la capacidad de comprensión del contexto se disparó, con algoritmos como BERTopic que aprovechan este avance para realizar tópicos que capturen una mayor complejidad.

-   Sin embargo, los LLMs llevan esto a otro nivel, permitiendo realizar análisis profundo, cadenas de pensamiento, codificaciones y mapear redes entre actores. Softwares como Atlas.ti, ampliamente utilizado para el análisis cualitativo, ya integraron la codificación automática con GPT en sus útimas versiones.

-   La literatura reciente ha demostrado formas innovadoras de utilizar estas herramientas para diferentes técnicas de análisis cualitativo:

    -   Para realizar análisis de contenido [@bijker2024]

    -   Para mapear redes entre actores [@bro2025]

    -   Para realizar análisis de discurso [@zhang2025]

    -   Para teoría fundamentada [@yue2025]

-   Según @hayes2025 los LLMs tienen un potencial revolucionario para la investigación cualitativa, donde los investigadores pueden "conversar" con los datos y probar conexiones teóricas que podrían tardar meses.

-   Estas potencialidades, en vez de reemplazar nuestras habilidades, las potencian, otorgándonos nuevas herramientas, mientras que el rol del sujeto se mantiene central en la parametrización, diseño de prompts, evaluación de outputs y creación de los marcos teóricos y operacionalizaciones adecuadas.

-   La adaptación de los métodos y su validación es algo que está recién comenzando, y mientras que algunos estudios han destacado una buena correspondencia entre los códigos que realizan los LLM y los humanos, se han presentado limitaciones y recomendaciones

-   **Limitaciones**

1.  Los códigos pueden ser más generales y omitir temas sensibles
2.  Los LLMs tienen el riesgo de alucinar (aunque esto ha reducido considerablemente en el último tiempo)
3.  Tienen una mayor dificultad de capturar tendencias generales

-   **Recomendaciones**

1.  Ingeniería de prompts precisa y bien pensada para el caso de uso específico
2.  Parametrización y selección correcta de modelos
3.  Colaboración y validación constante entre el investigador y el LLM, este es una herramienta útil, no reemplaza el trabajo.

## Contenidos del taller

En base a esto, el objetivo principal del taller es presentar un flujo de trabajo innovador para codificación cualitativa con LLMs, con un rol central del investigador en todo el proceso, y que puedan adaptar a múltiples casos de uso académicos y profesionales y distintas disciplinas. La gran mayoría de los análisis conducidos serán deductivos, para aplicar marcos teóricos relevantes y reducir la posibilidad de alucinación a través de prompting preciso.

Como ejercicio práctico, utilizaremos noticias chilenas para buscar los encuadres mediáticos (*framing)*, o constructos sociales sobre la IA en medios chilenos populares. La unidad de análisis serán las noticias completas, pero podríamos realizar el mismo ejercicio separando por frases o por párrafos realizando un preprocesamiento de los datos. Esto se hace para reducir los tiempos de espera y la complejidad (muchos datos anidados). Este taller asume que tienen un conocimiento básico-intermedio de R, manejando tidyverse y manejo de dataframes y listas básico, y aunque intentaremos ser lo más explicativos, nos centraremos en lo más sustantivo.

-   Configuración de la sesión y preparaciones necesarias

-   Diseño y optimización de prompts, creación de criterios para evaluar los resultados de los LLMs

-   Comprensión básica de los principales parámetros de los modelos

-   Creación de una función para aplicar el análisis y automatización de esta a lo largo de un dataframe. Utilizaremos Gemini ya que otorga créditos gratis de la API.

-   Revisión de los resultados y estrategias de validación

-   Análisis y estrategias de reporte

Si no tienen una API KEY o tienen algún problema, hablen por el chat y les ayudaré. Ahora Exequiel presentará la primera parte aplicada del taller.

## Configuración de la sesión

```{r}
pacman::p_load(datamedios, dplyr, gemini.R, ggplot2, jsonlite, lubridate, purrr, stringr, tidyverse, tidytext, tm, wordcloud2) # Cargamos paquetes necesarios
options(scipen = 999) # Desactivamos notación científica
rm(list = ls()) # Limpiamos entorno de trabajo
```

## Procesamiento y carga de los datos

Nota: Por si acaso alguien tiene algún error inesperado para utilizar datamedios, preparemos un csv con la data ya extraída para que lo utilicen.

Cargamos los datos del paquete datamedios (<https://exetrujillo.github.io/datamedios/>).

```{r}
datos <- datamedios::extraer_noticias_fecha("inteligencia artificial", "2025-11-1", "2025-11-25", subir_a_bd = FALSE)

# Si no te funciona datamedios puedes usar esto. Debes saltarte los siguientes pasos hasta el muestreo.
# noticias_muestra <- read.csv("noticias_muestra.csv")
```

Limpiamos los datos y seleccionamos las columnas de interés.

```{r}
datos <- datamedios::limpieza_notas(datos, sinonimos= c("IA", "chat gpt", "openai", "chatbot", "chatgpt"))

datos_filtrados <- datos %>%
    select(titulo, contenido_limpio, fecha, medio, url)
```

## Explorar data

```{r}
datamedios::grafico_notas_fecha(datos_filtrados, "Inteligencia artificial", tema = "dark")
```

Seleccionamos una submuestra de 20 noticias para reducir el tiempo de procesamiento de los análisis.

```{r}
set.seed(123)

noticias_muestra <- slice_sample(datos_filtrados, n=20)
```

Exploramos el dataframe de noticias y guardamos los datos brutos de la muestra.

```{r}
head(noticias_muestra, 5)
```

```{r}
#| eval: false
write.csv(noticias_muestra, "noticias_muestra.csv")
```

## Configuramos Gemini

Vamos a utilizar \`gemini.R\` [@kim2025], un paquete que nos abstrae de utilizar la API directamente, con funciones construidas útiles para hacer el análisis.

Cargamos las variables de entorno, en particular la API key de Gemini. Deben crear un archivo `.Renviron` y pegar lo siguiente:
```
GEMINI_API_KEY =tuapikeyaqui
(espacio en blanco)
```

Además hacemos un test simple. Si recibimos la respuesta de Gemini entonces podemos pasar a lo siguiente.

```{r}

readRenviron(".Renviron")
setAPI(Sys.getenv("GEMINI_API_KEY")) # check https://aistudio.google.com/api-keys
gemini("¿Cuál es la diferencia entre la inteligencia artificial y la inteligencia artesanal? Responde en una línea")
```

## Parámetros de gemini (y casi todos los LLM)

Configuramos los parámetros para el uso de la API.

-   **g_model:** En model se pueden usar varios para respuesta de texto, como "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite", etc. Ahora está disponible la familia gemini 3. Pueden revisar el listado completo [aquí](https://ai.google.dev/gemini-api/docs/models).
-   **g_temperature:** Podemos elegir también la temperatura para el modelo. Apunta a la objetividad-creatividad permitida en las respuestas. Números más cercanos a cero serían más "objetivos" y más conservadores (lo que a veces es menos útil), números más cercanos a dos serían más "creativos".
-   **g_seed:** También podemos setear una semilla para hacer los resultados más reproducibles aunque no podemos asegurarlo completamente.
-   Existen otros parámetros que para este caso no recomendamos cambiarlos, sin embargo, su información puede encontrarse en la documentación de la API de Gemini.

```{r}
# Parámetros configurables
g_model = "2.5-flash-lite"
g_temperature = 0.7
g_seed = 42

# Los siguientes recomendamos no cambiarlos para pruebas básicas
# maxOutputTokens = 8192
# topK = 40
# topP = 0.95
# timeout = 60
```

## Prompts

Cuando diseñamos prompts buscamos, por lo general, la respuesta a cierta pregunta o la resolución de algún problema. La forma en la que diseñamos un prompt influye estructuralmente en la respuesta que podamos recibir de un modelo de LLM.

En estricto rigor, el paquete que utilizamos no permite que le pasemos instrucciones de sistema, petición de texto y estructuras de respuesta por separado. Nosotros en este caso los separamos en objetos distintos solo para hacerlo más modular. Tenemos en este caso 3 partes, para comenzar a moldear las respuestas dependiendo de nuestro caso específico de estudio:

1.  **system_prompt:** Define el rol y forma de respuesta general del LLM. La idea es darle una idea específica de como "qué tipo de persona" tiene que comportarse y qué campos de conocimientos debe manejar para responder (es distinto preguntarle de inteligencia artificial a un experto en ciencias de la computación o a un periodista). Podemos utilizar este recurso para delimitar ciertos marcos teóricos y disciplinares al momento de esperar una respuesta.

2.  **prompt_instruccion:** Como su nombre lo dice, las instrucciones específicas. En nuestro caso, aquí es donde operacionalizamos las dimensiones analíticas. Debemos indicarle explícitamente al modelo qué conceptos teóricos debe buscar. Aquí traducimos conceptos abstractos (como la construcción social o el encuadre mediático) en directrices concretas: evaluar si la tecnología se presenta como amenaza u oportunidad (tecnofobia/tecnofilia), identificar qué actores tienen voz y detectar los sesgos latentes. Básicamente, esta sección funciona como nuestro libro de códigos o pauta de análisis cualitativo, asegurando que el modelo no divague, sino que interrogue al texto periodístico con un propósito definido.

3.  **json_estructura:** Finalmente, definimos el formato de salida. Aunque el análisis sea cualitativo, necesitamos sistematizar la información para poder trabajarla a escala en R. Al exigir una respuesta en formato JSON estricto, forzamos al modelo a sintetizar su razonamiento complejo en una estructura de datos legible por la máquina. En otras palabras, podremos transformar cientos o miles de análisis de texto individuales en un dataframe. Gracias a esto, podremos posteriormente calcular frecuencias, encontrar relaciones entre variables (por ej. *¿un medio es más tecnofílico que otro?*) y utilizar técnicas de Procesamiento de Lenguage Natural para identificar hallazgos significativos.

Nuestro propósito aquí es mostrar la estructura que usamos para que un LLM siga nuestras instrucciones como queremos, pero para cada caso de uso el tipo de prompt, los marcos teóricos, preguntas específicas y estructura de respuesta que pidamos será distinta.

```{r}
system_prompt <- "Eres un científico social experto en análisis cualitativo con formación interdisciplinaria en sociología, antropología, ciencias políticas y ciencias de la comunicación. Tienes amplia experiencia en investigación de medios, análisis de discurso, estudios culturales y semiótica social. Posees la capacidad de examinar contenido mediático desde múltiples perspectivas teóricas, identificando marcos narrativos e interpretativos, agendas mediáticas, construcciones simbólicas y procesos de significación. Mantienes una perspectiva académica rigurosa y pensamiento crítico interdisciplinario, fundamentando tus análisis en la evidencia proporcionada."

prompt_instruccion <- "Analiza la siguiente noticia relacionada con Inteligencia Artificial (IA) desde una perspectiva de ciencias sociales y comunicación.

Requisitos para el análisis:
- Evalúa la centralidad de la IA en la narrativa.
- Identifica orientaciones tecnofóbicas, neutrales o tecnofílicas.
- Examina cómo se representa la IA: capacidades, riesgos, limitaciones, beneficios.
- Detecta sesgos, omisiones o énfasis en el discurso.
- Identifica los actores sociales mencionados y clasifícalos.

Clasifica a cada actor en una de las siguientes categorías:
    1. Empresa Tecnológica (Ej: OpenAI, Google)
    2. Empresa No Tecnológica (Ej: Retail, Minería)
    3. Finanzas (Ej: Bancos, Fondos de inversión)
    4. Sector Público (Ej: Ministerios, Agencias)
    5. Políticos (Ej: Legisladores, Presidentes)
    6. Academia / Expertos (Ej: Universidades, Investigadores)
    7. Líder Empresarial (Personas: CEOs, Fundadores)
    8. Sociedad Civil (ONGs, Ciudadanía)

Genera un archivo JSON con los resultados."

json_estructura <- 'Responde exclusivamente en formato JSON válido. Tu respuesta debe ser ÚNICAMENTE el objeto JSON, comenzando con "{" y terminando con "}". 

Consideraciones:
  - Las keywords deben ser exactamente 5, en minúsculas.
  - Actores: Identifica HASTA 5 actores principales. Si hay menos, no rellenes con información falsa ni NULLs innecesarios, simplemente cierra la lista.
  - Label: Máximo 3 palabras. Si la IA no es central, el label debe reflejar el tema real.

Estructura requerida:
{
  "keywords": ["palabra1", "palabra2", "palabra3", "palabra4", "palabra5"],
  "orientacion_tecnologica": "tecnofobia|tecnofilia|neutralidad",
  "centralidad_ia": "alta|media|baja",
  "actores_principales": [
    {
      "nombre": "nombre_exacto_en_texto",
      "categoria": "Categoría Seleccionada",
      "rol_respecto_ia": "descripción breve de su posición"
    }
  ],
  "analisis": "Análisis denso de 4 a 6 oraciones sintetizando los hallazgos sociológicos, el tono y los marcos interpretativos detectados.",
  "label": "etiqueta_breve"
}
'
```

## Función para crear prompt pasándole las noticias

Aquí concatenamos las partes del prompt que hicimos antes y le entregamos un *input* estructurado al LLM, pasándole el título y el contenido de la noticia. Al final imprimimos el resultado del prompt para una sola fila.

```{r}
# Crear prompt completo con contexto
crear_prompt_completo <- function(fila_noticia) {
  # Extraer información de la fila
  titulo <- fila_noticia$titulo
  contenido <- fila_noticia$contenido_limpio
  fecha <- fila_noticia$fecha
  
  # Crear contexto de la noticia
  contexto_noticia <- paste0(
    "# CONTEXTO DE LA NOTICIA:\n",
    "Título: ", titulo, "\n",
    "CONTENIDO:\n", contenido
  )
  
  # Combinar todos los elementos del prompt
  prompt_final <- paste(
    system_prompt,
    "\n\n",
    prompt_instruccion,
    "\n\n",
    contexto_noticia,
    "\n\n",
    json_estructura,
    sep = ""
  )
  
  return(prompt_final)
}

# Aquí ejecutamos
prompt_example <- crear_prompt_completo(noticias_muestra[6,])
example <- gemini(
  prompt_example,
  model = g_model,
  temperature = g_temperature,
  seed = g_seed
)

cat(example)
print(noticias_muestra$contenido_limpio[6])
```

Analicemos la respuesta: ¿Hay algún problema que detectemos? ¿Algo que no funcione como esperábamos? El prompt final que obtuvimos es el resultado de un proceso de prueba y error, en donde diferentes ajustes fueron realizados. Deberíamos observar varias noticias distintas para ver como funciona, y probar si en distintos intentos el modelo está respondiendo de forma similar. Si queremos que se centre en otras facetas del discurso, lo esencial está en esta etapa de evaluación, en donde la meta es asegurarnos que el LLM esté logrando cubrir nuestros objetivos de investigación de forma satisfactoria.

## Iteración guardando en una lista

Iteramos por cada una de las filas del dataframe para obtener nuestra respuesta estructurada. Guardamos esto en una lista manejando los errores que puedan surgir. A veces, trabajando con API, una fila va a tener un error y nos puede detener el trabajo subsiguiente. Para ello implementamos una lógica de reintentos.

```{r}
lista_respuestas <- list()

lista_respuestas <- vector("list", nrow(noticias_muestra)) # Creamos posiciones vacías en la lista para cada fila de nuestro dataframe

for (i in 1:nrow(noticias_muestra)){
  noticia <- noticias_muestra[i,]
  prompt <- crear_prompt_completo(noticia)
  
  analisis <- NULL

  # Lógica de reintentos
  for(intento in 1:3){ 
    analisis <- tryCatch(
      gemini(prompt, model = g_model, temperature = g_temperature, seed = g_seed),
      error = function(e) { message(paste("Error en fila", i, "intento", intento)); return(NULL) }
    )
    if(!is.null(analisis)) break 
    Sys.sleep(2)
  }  
  if(!is.null(analisis)) {
    lista_respuestas[[i]] <- analisis
  } else {
    lista_respuestas[[i]] <- NA 
    message(paste("Fila", i, "quedó vacía por error en API."))
  }
  
  Sys.sleep(2)
}
```

Revisamos el caso de la sexta fila, que debiese ser muy parecido al del ejemplo inicial. Un tip: para ahorrar recursos, podemos realizar un análisis a una menor escala, ver si funciona correctamente, y luego ejecutarlo en todos los datos. Para la versión final, también podríamos utilizar un modelo de razonamiento más caro (`gemini 3 pro`), para asegurarnos que los resultados definitivos sean los mejores.

```{r}
cat(lista_respuestas[[6]])
```

## Limpieza y aplanamiento

Aplicamos una función de limpieza a la lista para eliminar los elementos *markdown* que nos entregó el LLM en la respuesta. Como *output* tendremos una lista concatenada con los distintos elementos que le solicitamos y lista para usar en R.

```{r}
lista_proc <- lapply(lista_respuestas, function(texto_sucio) {
  texto_limpio <- str_trim(str_remove_all(texto_sucio, "^```json|```$"))
  objeto_r <- fromJSON(texto_limpio, flatten = TRUE)
  return(objeto_r)
})

print(lista_proc[[1]])
```

Ya tenemos nuestro objeto de listas concatenadas, pero también podemos crear un dataframe para operarlo de mejor manera en R. En este caso tenemos una estructura algo compleja, ya que tenemos una lista que dentro también tiene un dataframe (con los actores y su rol). Entonces, creamos un dataframe aplanando nuestra lista.

```{r}
aplanar_elemento <- function(elemento) {
  data.frame(
    # Vectores simples
    keywords = paste(elemento$keywords, collapse = "; "),
    orientacion_tecnologica = elemento$orientacion_tecnologica,
    centralidad_ia = elemento$centralidad_ia,
    label = elemento$label,
    analisis = elemento$analisis,
    
    # Información de actores (concatenamos todo)
    actores_nombres = paste(elemento$actores_principales$nombre, collapse = "; "),
    actores_categorias = paste(elemento$actores_principales$categoria, collapse = "; "),
    actores_roles = paste(elemento$actores_principales$rol_respecto_ia, collapse = "; "),
    
    stringsAsFactors = FALSE
  )
}

# Aplicar a todos los elementos
df_proc <- purrr::map_dfr(lista_proc, aplanar_elemento)
```

Finalmente podemos unificar en un dataframe para tener la información original y la de análisis en un mismo lugar

```{r}
datos_unificados <- bind_cols(noticias_muestra, df_proc)
```

Para la información de los actores, guardaremos los dataframes que tenemos dentro de las listas en formato long junto a información de las noticias.

```{r}

extraer_actores_long <- function(i, noticias, lista_json) {
  info_noticia <- noticias[i, ] %>% 
    select(titulo, medio, url)
  elemento_llm <- lista_json[[i]]
  
  actores_df <- elemento_llm$actores_principales
  # Si no hay actores o está vacío, saltamos esta noticia
  if (is.null(actores_df) || length(actores_df) == 0 || nrow(actores_df) == 0) {
    return(NULL)
  }
  
  actores_df$orientacion_tecnologica <- elemento_llm$orientacion_tecnologica

  actores_df$analisis <- elemento_llm$analisis

  resultado <- bind_cols(info_noticia, actores_df)
  
  return(resultado)
}

df_actores_unificados <- purrr::map_dfr(
  1:nrow(noticias_muestra), 
  ~extraer_actores_long(.x, noticias_muestra, lista_proc)
)

head(df_actores_unificados)

```

Y podemos guardar los datos (●'◡'●)

```{r}
#| eval: false

write.csv(datos_unificados, "datos_analizados.csv")
write.csv(df_actores_unificados, "datos_actores.csv")
# Aquí también podemos importar el csv
# datos_unificados <- read.csv("datos_analizados.csv")
```

## Análisis y estrategias de reporte

Por motivos pedagógicos, vamos a cargar una base de datos más grande preprocesada por gemini (n=252), seleccionando la misma cantidad de noticias de tres medios distintos entre fines del 2022 y 2025.

Uno de los principales desafíos al utilizar estas herramientas es poder entender el análisis realizado. Para esto, podemos utilizar diversas visualizaciones y Procesamiento de Lenguaje Natural para identificar conceptos clave, y posteriormente profundizar o "hacer zoom" en los puntos más relevantes para nuestros objetivos.

```{r}
datos_descriptivos <- read.csv("datos_analisis.csv")
datos_actores <- read.csv("datos_actores_analisis.csv")
```

Observamos la frecuencia de los datos a lo largo del tiempo. Van aumentando desde la salida de ChatGPT.
```{r}
datamedios::grafico_notas_fecha(datos_descriptivos, titulo="Noticias a lo largo del tiempo en los datos", agrupar_por = "month", tema = "dark")
```

### Preprocesamiento

```{r}

df <- datos_descriptivos %>%
  mutate(
    fecha = as.Date(fecha), # Convertimos a fecha
    centralidad_ia = factor(centralidad_ia, levels = c("baja", "media", "alta")),
    orientacion_tecnologica = factor(orientacion_tecnologica, 
                                     levels = c("tecnofobia", "neutralidad", "tecnofilia"))
  )
colores_orientacion <- c("tecnofobia" = "#D73027",
                         "neutralidad" = "#E0E0E0",
                         "tecnofilia" = "#4575B4")
                      
colores_centralidad <- c("baja" = "#7008E7", 
                         "media" = "#2AA63E", 
                         "alta" = "#FE9A37")
```

### Frecuencia de orientación tecnológica

```{r}

grafico_1 <- df %>%
  count(orientacion_tecnologica) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = orientacion_tecnologica, y = n, fill = orientacion_tecnologica)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), vjust = -0.5) +
  scale_fill_manual(values = colores_orientacion) +
  labs(title = "Distribución de la Orientación Tecnológica",
       subtitle = "¿Cómo se evalúa la IA en las noticias?",
       x = NULL, y = "Cantidad de noticias") +
  theme_minimal()

grafico_1

```

La orientación más presente en los medios es *neutralidad*, seguida de *tecnofilia*. Existe una baja proporción de noticias con una concepción negativa de la IA, de solo 12%.

### Frecuencia de centralidad de la IA

```{r}
grafico_2 <- df %>%
  count(centralidad_ia) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = centralidad_ia, y = n, fill= centralidad_ia)) +
  geom_col() + 
  scale_fill_manual(values = colores_centralidad) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), vjust = -0.5) +
  labs(title = "Centralidad de la IA en el discurso",
       subtitle = "Nivel de protagonismo de la tecnología en la noticia",
       x = "Nivel de Centralidad", y = "Cantidad de noticias") +
  theme_minimal()

grafico_2
```

La centralidad de la IA es comúnmente alta, con una baja proporción de noticias en donde es una temática pasajera.

### Orientación tecnológica a través del tiempo

```{r}
df_tiempo_orientacion <- df %>%
  mutate(anio = as.factor(year(as.Date(fecha)))) %>%
  count(anio, orientacion_tecnologica) %>%
  group_by(anio) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

grafico_4 <- df_tiempo_orientacion %>%
  ggplot(aes(x = anio, y = prop, fill = orientacion_tecnologica)) +
  geom_col(position = "stack", width = 0.7) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), 
            position = position_stack(vjust = 0.5), # Centrado en la barra
            color = "white", 
            fontface = "bold",
            size = 4) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_orientacion) +
  labs(title = "Evolución de la Orientación Tecnológica de los medios por Año",
       subtitle = "Proporción de noticias según Tecnofobia, Neutralidad o Tecnofilia",
       x = "Año",
       y = "Porcentaje",
       fill = "Orientación") +
  theme_minimal() +
  theme(
    legend.position = "top",
    panel.grid.major.x = element_blank()
  )

grafico_4
```

En 2022 tenemos muy pocas noticias. El 2024 fue el año con una narrativa más positiva de la IA, mientras que en 2025 se presenta menor tecnofilia y mayor tecnofobia. Podríamos explorar si esto se debe a las noticias recientes sobre la burbuja financiera.

### Centralidad IA a través del tiempo

```{r}

df_tiempo_centralidad <- df %>%
  mutate(anio = as.factor(year(as.Date(fecha)))) %>%
  count(anio, centralidad_ia) %>%
  group_by(anio) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

grafico_4 <- df_tiempo_centralidad %>%
  ggplot(aes(x = anio, y = prop, fill = centralidad_ia)) +
  geom_col(position = "stack", width = 0.7) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), 
            position = position_stack(vjust = 0.5), # Centrado en la barra
            color = "white", 
            fontface = "bold",
            size = 4) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_centralidad) +
  labs(title = "Evolución de la Centralidad de la IA por Año",
       subtitle = "Proporción de noticias según nivel de protagonismo",
       x = "Año",
       y = "Porcentaje",
       fill = "Centralidad") +
  theme_minimal() +
  theme(
    legend.position = "top",
    panel.grid.major.x = element_blank()
  )

grafico_4


```

Asimismo, la centralidad de la IA baja durante los años. Podríamos pensar que mientras se va normalizando esta tecnología, aparece en más noticias como un tema secundario, mientras que al comienzo era el principal foco de atención.

### Orientación tecnológica por medio

```{r}
grafico_5 <- df %>%
  count(medio, orientacion_tecnologica) %>%
  group_by(medio) %>%
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = medio, y = pct, fill = orientacion_tecnologica)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_orientacion) +
  coord_flip() +
  labs(title = "Orientación Tecnológica por Medio",
       subtitle = "Proporción de noticias positivas, neutras y negativas",
       x = NULL, y = "Porcentaje", fill = "Orientación") +
  theme_minimal()

grafico_5
```

Vemos tendencias muy marcadas por medio. Mientras que Emol y medios regionales (ambos de El Mercurio) presentan una mucho mayor tecnofilia, bío bío se posiciona de forma más neutral en las noticias, con discursos más criticos frente a los otros medios. 

### Centralidad IA por medio

```{r}
grafico_6 <- df %>%
  count(medio, centralidad_ia) %>%
  group_by(medio) %>%
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = medio, y = pct, fill = centralidad_ia)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_centralidad) +
  coord_flip() +
  labs(title = "Centralidad IA por Medio",
       subtitle = "Proporción de noticias positivas, neutras y negativas",
       x = NULL, y = "Porcentaje", fill = "Orientación") +
  theme_minimal()

grafico_6
```

De forma consistente, Emol y medios regionales suelen dar un rol central a la IA en sus noticias, mientras que en bío bío no se observa una tendencia clara.

### Análisis de actores

#### Frecuencia de actores
```{r}
grafico_actores_freq <- datos_actores %>%
  count(categoria) %>%
  mutate(categoria = fct_reorder(categoria, n)) %>%
  ggplot(aes(x = categoria, y = n)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = n), hjust = -0.5, size = 3.5) +
  coord_flip() +
  labs(title = "Actores predominantes en las noticias sobre IA",
       subtitle = "Frecuencia de mención por categoría",
       x = NULL, y = "Número de menciones") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())

print(grafico_actores_freq)
```

Sobre los actores, vemos que las Empresas tecnológicas y los expertos están muy presentes en los discursos, mientras que la sociedad civil y los políticos tiene un rol bastante menor en la prensa. Aquí observamos algo interesante: La IA alucinó dos categorías, N/A y medio de comunicación. Ahora deberíamos revisar esos casos, volver a la instrucción, precisar más el promt (quizás agregando los medios de comunicación explícitamente dentro de las empresas no tecnológicas) y volver a correr el análisis.

```{r}
print(datos_actores |> filter(categoria=="N/A") |> select(nombre, titulo))

print(datos_actores |> filter(categoria %in% c("Medio de Comunicación", "Medios de Comunicación")) |> select(nombre, titulo))

print(datos_actores |> filter(str_detect(nombre, "AFP")) |> select(nombre, categoria, titulo))
```

Para N/A, no está pudiendo categorizar a autores de libros. Podríamos añadir una categoría de artistas, que no habíamos considerado por solitario. Para los medios, todos son radios o periódicos, exceptuando AFP y EFE. Deberíamos desarrollar una estrategia para estas. Agregaremos medio a Empresa no Tecnológica y crearemos manualmente la categoría de artistas y cultura.

```{r}
datos_actores <- datos_actores |> 
  mutate(categoria = case_when(
    categoria %in% c("Medio de Comunicación", "Medios de Comunicación") ~ "Empresa No Tecnológica",
    categoria == "N/A" ~ "Artistas o cultura",
        TRUE ~ categoria
  ))


```

#### Relación entre tipos de actores y orientación tecnológica
```{r}
grafico_actores_orientacion <- datos_actores %>%
  # Filtramos categorías con muy pocos casos para limpiar el gráfico (opcional)
  add_count(categoria) %>%
  filter(n >= 3) %>% 
  
  count(categoria, orientacion_tecnologica) %>%
  group_by(categoria) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup() %>%
  
  ggplot(aes(x = categoria, y = prop, fill = orientacion_tecnologica)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_orientacion) +
  coord_flip() +
  labs(title = "Orientación Tecnológica por Tipo de Actor",
       subtitle = "Cómo se asocian los distintos actores a las narrativas (Tecnofilia vs Tecnofobia)",
       x = NULL, y = "Proporción",
       fill = "Orientación") +
  theme_minimal() +
  theme(legend.position = "top")

print(grafico_actores_orientacion)
```

Vemos dos tendencias muy marcadas: Sociedad civil se relaciona con noticias más tecnofóbicas, y las categorías empresariales con la tecnofilia. 

#### Actores más mencionados
```{r}

top_nombres <- datos_actores %>%
  filter(!is.na(nombre)) %>%
  count(nombre, sort = TRUE) %>%
  head(10)

print(top_nombres)

```

### Nube de palabras columna análisis

```{r}
# Stop words

lista_base <- tibble(word = tm::stopwords("es"))

mas_stop_words <- tibble(word = c(
  "inteligencia", "artificial", "noticia", "noticias",
  "orientación", "tecnológica", "ia", "dentro", "destacando",
  "enmarca", "centra", "clara", "discurso", "si", "énfasis",
  "forma", "presenta", "narrativa", "través", "figura", "factor",
  "sino", "busca", "chile", "abordar", "chileno", "sector",
  "actores", "apoyo", "oscila", "enfatizan", "generar",
  "ser", "allá", "análisis", "actúa", "hacia", "tecnología",
  "observa", "elemento", "foco", "cómo", "marco", "aborda", "utiliza", "posibles", "enmarcando", "utilizando", "construcción", "discursiva", "marcos", "narrativos",
  "principal", "recae", "presentando", "informativo", "tratamiento", "agenda", "mediática",
  "eje", "central", "tema", "predominantemente", "asociados", "omitiendo", "manera", "mencionada", "aunque", "torno", "enfoca", "interpretativo", "mención", "tono", "falta", "tecnológico", "práctica", "presentándola", "posicionando", "observan", "medio", "ámbito", "fuertemente", "interpretativos", "tecnofílicas", "tecnofílico", "tecnofílica", "tecnológicas", "posiciona", "marcadamente", "empresarial"
))

stop_words_final <- bind_rows(lista_base, mas_stop_words)

palabras_para_nube <- df %>%
  unnest_tokens(word, analisis) %>%
  mutate(word = str_trim(word)) %>%
  anti_join(stop_words_final, by = "word") %>%
  count(word, sort = TRUE) |> 
  slice_max(order_by = n, n = 70)

wordcloud2(palabras_para_nube, size = 0.7, color = "random-dark")

```

Vemos múltiples palabras relacionadas al deasrrollo y una visión positiva de la IA: innovación, motor, eficiencia, avance, desarrollo, etc. Por otro lado, la palabra riesgos es la más mencionada en el análisis, junto a limitaciones y desafíos por el lado de los discursos más pesimistas. Hay distintas palabras temáticas presentes, cómo éticos, laboral, sociales, mercado, humano, seguridad, gestión. 

### Bigramas análisis

```{r}
# Tokenizar en pares de palabras (bigramas)
bigramas_analisis <- df %>%
  unnest_tokens(bigram, analisis, token = "ngrams", n = 2) %>%
  # Separar para filtrar stopwords
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words_final$word,
         !word2 %in% stop_words_final$word) %>% 
  # Unir de nuevo
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

# Graficar los Top 15 bigramas
grafico_6 <- bigramas_analisis %>%
  head(20) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = bigram, y = n)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(title = "Frases más comunes en el análisis cualitativo",
       subtitle = "Bigramas más frecuentes",
       x = NULL, y = "Frecuencia") +
  theme_minimal()

print(grafico_6)
```

Observando el análisis de los bigramas, se obtiene una mirada más profunda del análisis realizado por el LLM. Podemos ver conceptos como "Herramienta fundamental", "crecimiento económico" o "beneficios potenciales" cómo temáticas que uno podría asociar más a la tecnofilia. Por el otro lado, se destaca el discurso negativo, con frases como "debates éticos" e "implicaciones sociales". Podemos mirar más bigramas si nos interesan más conceptos específicos.

Por ejemplo, me interesan ver más bigramas sobre la orientación tecnofóbica, ya que al ser menos frecuente el gráfico la infrarrepresenta.

```{r}
# Tokenizar en pares de palabras (bigramas)
bigramas_analisis <- df %>%
  filter(orientacion_tecnologica=="tecnofobia") |> 
  unnest_tokens(bigram, analisis, token = "ngrams", n = 2) %>%
  # Separar para filtrar stopwords
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words_final$word,
         !word2 %in% stop_words_final$word) %>% 
  # Unir de nuevo
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

# Graficar los Top 15 bigramas
grafico_7 <- bigramas_analisis %>%
  head(20) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = bigram, y = n)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(title = "Frases más comunes en el análisis cualitativo",
       subtitle = "Bigramas más frecuentes tecnofobia",
       x = NULL, y = "Frecuencia") +
  theme_minimal()

print(grafico_7)
```

Vemos frases como sociedad civil, fuerza disruptiva o graves concecuencias. Podríamos ver también las palabras más frecuentes.  Aparece arte, pérdida, creación y seguridad.

```{r}
bigramas_analisis <- df %>%
  filter(orientacion_tecnologica=="tecnofobia") |> 
  unnest_tokens(bigram, analisis, token = "ngrams", n = 1) %>%
  # Separar para filtrar stopwords
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words_final$word,
         !word2 %in% stop_words_final$word) %>% 
  # Unir de nuevo
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

# Graficar los Top 15 bigramas
grafico_7 <- bigramas_analisis %>%
  head(20) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = bigram, y = n)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(title = "Conceptos más comunes en el análisis cualitativo",
       subtitle = "Palabras más frecuentes",
       x = NULL, y = "Frecuencia") +
  theme_minimal()

print(grafico_7)
```

Tanto en los bigramas como en la nube de palabras deberíamos ir agregando palabras que generen ruido, como "enfatiza" en este caso.

### Búsqueda de casos relevantes

Comúnmente, para reportar el análisis cualitativo y revisar la calidad de los análisis vamos a necesitar detectar donde aparecen estos conceptos clave en el análisis del LLM. De esta forma, podremos encotrar citas relevantes para evidenciar las narrativas que sustentan estos conceptos. Para esto, creamos una función sencilla de búsqueda.

```{r}
concepto <- "crecimiento económico"

busqueda <- df %>%
  filter(str_detect(analisis, concepto))

print(busqueda)
```

Ahora, podemos revisar algúna(s) noticia que nos llame la atención particularmente. Podremos hacer esto con cada concepto clave en nuestro análisis. Podríamos ver también si estas noticias se asocian con una orientación tecnológica específica.

```{r}
concepto <- "laboral"

busqueda <- df %>%
  filter(str_detect(analisis, concepto)) |> 
  select(titulo, url, orientacion_tecnologica, analisis)

summary(busqueda$orientacion_tecnologica)

print(busqueda[22, ])
```

Llamativamente, las noticias con el concepto laboral no tienen una marcada tecnofobia, pero revisando hay una que habla del reemplazo en de los trabajos. Podríamos centrarnos en esta destacando la gran falta de esta narrativa en los medios.

También, podemos hacer búsquedas complejas. Por ejemplo, si quiero encontrar noticias de emol, con presencia de la sociedad civil y tecnofobia.

```{r}

busqueda <- datos_actores %>%
  filter(medio == "emol") |> 
  filter(categoria == "Sociedad Civil") |> 
  filter(orientacion_tecnologica == "tecnofobia")

print(busqueda)
```

O si quiero ver la aparición de un concepto y tipo de actor (incluso podría buscar por nombre).
```{r}
concepto <- "debates éticos"

busqueda <- datos_actores %>%
  filter(str_detect(analisis, concepto)) |> 
  filter(categoria == "Academia / Expertos")

print(busqueda)

```

Hay muchisimas formas de explorar los resultados. El eje orientador son nuestros objetivos, nuestro marco teórico y nuestra imaginación. Podríamos utilizar algún modelo como tf_idf para ver conceptos más relevantes cuando aparecen ciertos actores o cuando hay cierta orientación tecnológica. También, podríamos realizar bigramas o palabras más frecuentes según tipo de actor, centralidad, medio, etc.

Al final de este análisis, me dieron ganas de añadir un campo de "Temática", para capturar si la noticia trata de economía, política, cultura, educación, etc. Este tipo de decisiones son clave para enriquecer los análisis, y comúnmente se nos ocurren luego de empezar a explorar los datos.

## Conclusión

-   Revisamos los avances recientes de la IA y sus aplicaciones al análisis de texto
-   Aprendimos un flujo de trabajo automatizado para realizar análisis cualitativo en grandes volúmenes de texto, aplicable para múltiples disciplinas y a través de diferentes modelos
-   Profundizamos en distintas prácticas para el correcto funcionamiento de estas tecnologías, como el uso de prompts precisos y estructurados, y el rol central del investigador en el proceso
-   Procesamos estos datos para poder analizarlos en R, y observamos diferentes alternativas básicas para explorar los datos, analizarlos y sacar conclusiones.

## Discusión

- Si bien aún queda mucho camino por recorrer, aprender a utilizar este tipo de herramientas presenta grandes ventajas para la investigación cualitativa
- Mientras que existen limitaciones, al tomar los LLM como una herramienta más en el proceso de investigación podemos potenciar nuestras herramientas de análisis, ahorrar tiempo y poder resolver nuevas preguntas
- Según nuestra experiencia, es más sencillo realizar esto con análisis deductivo, para reducir las alucinaciones y los posibles sesgos de los modelos y tener mayor injerencia en los resultados
- El aprendizaje de estas herramientas abre nuevos horizontes para la investigación cualitativa y nos permite entrar en una discusión en la que muchas veces no formamos parte: cómo es el proceso en que un LLM toma cierta decisión
- Por ello, es esencial utilizar estas herramientas desde plataformas que permitan configurar y parametrizar a nuestro gusto, no en los servicios que ofrecen las empresas
- Por último, un gran problema con estas herramientas es el uso de datos para el entrenamiento de los modelos. Ahora usamos datos abiertos de la web, pero si se cuenta con entrevistas personales hay que ser mucho más cuidadosos. Respecto a esto, hay dos soluciones:
1. Modelos open source que podemos utilizar desde infraestructura nuestra o en la nube (pero requiere capacidades técnicas mucho mayores)
2. Plataformas con políticas de recolección de datos Zero Data Retention. Generalmente, son servicios que se dedican a vender APIs, y no guardan los datos que mandas (existen certificaciones para esto). Encontramos dos que destacan:
https://openrouter.ai/docs/guides/features/zdr
https://docs.fireworks.ai/guides/security_compliance/data_handling