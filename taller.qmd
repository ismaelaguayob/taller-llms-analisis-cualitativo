---
title: "Taller"
subtitle: "Gemini para análisis cualitativo escalable en R"
author: "Ismael Aguayo y Exequiel Trujillo"
date: today
lang: en
fontsize: 14pt
format:
  html:
    code-fold: true
    toc: true
    toc-location: left
    toc-depth: 2
    toc-expand: 1
    toc-title: Contenidos
    number-sections: true
    theme: socialtec.scss
    code-link: true
    title-block-banner: true
bibliography: references.bib
csl: apa.csl
link-citations: true
editor_options: 
  chunk_output_type: console
---

## Antecedentes

### La revolución de la Inteligencia Artificial

-   *Attention is all you need* [@vaswani2017]: Artículo que crea la arquitectura transformer, que es la base de las IA actuales debido a su escalabilidad y eficiencia

-   Los grandes modelos de lenguaje (LLM por sus siglas en inglés) son modelos de Inteligencia Artificial entrenados en grandes volúmenes de datos, usualmente bajo la arquitectura de transformer, con la capacidad de comprender y generar texto para distintas tareas (traducción, programación, redacción, razonamiento, etc.)

-   30 de noviembre 2022: lanzamiento de ChatGPT. 100 millones de usuarios en un año.

-   Esto provocó un desarrollo acelerado de la IA en los últimos años:

    -   Áuge en la competencia y modelos open source: Claude, Gemini, Grok, Kimi-k2, Gemma, Gpt-Oss
    -   Múltiples aplicaciones: Vibe coding (Cursor, Windsurf), Ciencia (AlphaFold), Imágenes (MidJourney, NanoBanana), video (Veo3, Sora)
    -   Desarrollo de infraestructura clave: construcción de centros de datos y supercomputadores
    -   Avances en otras áreas como la robótica, la medicina, la neurociencia, etc.

### Herramientas de consumidor vs. herramientas de desarrollador

-   Dentro del mercado de la IA, existen distintos productos para distintas necesidades

-   Los servicios para los consumidores son los que comúnmente uno utiliza, los chatbot accesibles a través de las páginas web o aplicaciones de celular de ChatGpt o Gemini. Usualmente, tienen planes gratuitos y de pago.

![](images/chatgpt.jpg){fig-align="center"}

-   Por otro lado, están las plataformas para desarrolladores, comúnmente ofrecidas por las empresas a través de una Interfaz de Programación de Aplicaciones (API por sus siglas en inglés). A diferencia de los chatbots, las APIs cobran por cada llamada o mensaje enviado, y están diseñadas para poder desarrollar usos específicos de los LLM.

-   En este taller, utilizaremos este segundo enfoque, para aprovechar los beneficios que puede otorgar la Inteligencia Artificial, pero realizar los ajustes necesarios para mantener un flujo de investigación riguroso

### LLMs para análisis cualitativo

-   Anteriormente, la forma de utilizar *text-as-data* más común era a través de modelos más básicos, con una menor capacidad de comprender el contexto amplio. Por ejemplo, para identificar tipos de consumidores en un dataset de comentarios, se podía ejecutar un LDA o un STM para modelar los tópicos subyacentes.

-   Desde la arquitectura transformers, la capacidad de comprensión del contexto se disparó, con algoritmos como BertTopic que aprovechan este avance para realizar tópicos que capturen una mayor complejidad.

-   Sin embargo, los LLMs llevan esto a otro nivel, permitiendo realizar análisis profundo, cadenas de pensamiento, codificaciones y mapear redes entre actores. Softwares como Atlas.ti, ampliamente utilizado para el análisis cualitativo, ya integraron la codificación automática con GPT en sus útimas versiones.

-   La literatura reciente ha demostrado formas innovadoras de utilizar estas herramientas para diferentes técnicas de análisis cualitativo:

    -   Para realizar análisis de contenido [@bijker2024]

    -   Para mapear redes entre actores [@bro2025]

    -   Para realizar análisis de discurso [@zhang2025]

    -   Para teoría fundamentada [@yue2025]

-   La adaptación de los métodos y su validación es algo que está recién comenzando, y mientras que los estudios han destacado una buena correspondencia entre los códigos que realizan los LLM y los humanos, se han presentado grandes limitaciones y recomendaciones

-   **Limitaciones**

1.  Los códigos pueden ser más generales y omitir temas sensibles
2.  Los LLMs tienen el riesgo de alucinar (aunque esto ha reducido considerablemente en el último tiempo)
3.  Tienen una mayor dificultad de capturar tendencias generales

-   **Recomendaciones**

1.  Ingeniería de prompts precisa y bien pensada para el caso de uso específico
2.  Parametrización y selección correcta de modelos
3.  Colaboración y validación constante entre el investigador y el LLM, este es una herramienta útil, no reemplaza el trabajo.

## Contenidos del taller

-   Configuración de la sesión y preparaciones necesarias

-   Diseño y optimización de prompts, creación de criterios para evaluar los resultados de los LLMs

-   Comprensión básica de los principales parámetros de los modelos

-   Creación de una función para aplicar el análisis cualitativo y automatización de esta a lo largo de un dataframe. Utilizaremos Gemini ya que otorga créditos gratis de la API.

-   Análisis de la información obtenida

Todo el contenido del taller quedará a disposición posteriormente. El taller asume que tienen algunos conocimientos de R básicos-intermedios, cómo manejo de dataframes, funciones, iteraciones y el ecosistema tidyverse, por lo que no entraremos en mayores detalles sobre todos estos contenidos. El principal objetivo es que puedan realizar análisis cualitativo de forma automatizada utilizando LLMs a través de R, independientemente del área de estudios o el caso de uso que tengan, y logren adaptar las herramientas técnicas que les enseñaremos de forma sencilla.

## Configuración de la sesión

```{r}
pacman::p_load(datamedios, dplyr, gemini.R, ggplot2, jsonlite, lubridate, purrr, stringr, tidyverse, tidytext, tm, wordcloud2) # Cargamos paquetes necesarios
options(scipen = 999) # Desactivamos notación científica
rm(list = ls()) # Limpiamos entorno de trabajo
```

## Procesamiento y carga de los datos

Nota: Por si acaso alguien tiene algún error inesperado para utilizar datamedios, preparemos un csv con la data ya extraída para que lo utilicen.

Cargamos los datos del paquete datamedios (<https://exetrujillo.github.io/datamedios/>).

```{r}
datos <- datamedios::extraer_noticias_fecha("inteligencia artificial", "2025-11-1", "2025-11-25")

# Si no te funciona datamedios puedes usar esto
# noticias_muestra <- read.csv("noticias_muestra.csv")
```

Limpiamos los datos y seleccionamos las columnas de interés.

```{r}
datos <- datamedios::limpieza_notas(datos, sinonimos= c("IA", "chat gpt", "openai", "chatbot", "chatgpt"))

datos_filtrados <- datos %>%
    select(titulo, contenido_limpio, fecha, medio, url)
```

## Explorar data

```{r}
datamedios::grafico_notas_fecha(datos_filtrados, "Inteligencia artificial", tema = "dark")
```

Seleccionamos una submuestra de 20 noticias para reducir el tiempo de procesamiento de los análisis.

```{r}
set.seed(123)

noticias_muestra <- slice_sample(datos_filtrados, n=20)
```

Exploramos el dataframe de noticias y guardamos los datos brutos de la muestra.

```{r}
head(noticias_muestra, 5)
```
```{r}
#| eval: false
write.csv(noticias_muestra, "noticias_muestra.csv")
```

## Configuramos Gemini

Cargamos las variables de entorno, en particular la API key de Gemini. Además hacemos un test simple. Si recibimos la respuesta de Gemini entonces podemos pasar a lo siguiente.

```{r}

readRenviron(".Renviron")
setAPI(Sys.getenv("GEMINI_API_KEY")) # check https://makersuite.google.com/app/apikey
gemini("¿Cuál es la diferencia entre la inteligencia artificial y la inteligencia artesanal? Responde en una línea")
```

## Parámetros de gemini (y casi todos los LLM)

Configuramos los parámetros para el uso de la API.

-   **g_model:** En model se pueden usar varios para respuesta de texto, como "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite", etc. Pueden revisar el listado completo [aquí](https://ai.google.dev/gemini-api/docs/models).
-   **g_temperature:** Podemos elegir también la temperatura para el modelo. Apunta a la objetividad-creatividad permitida en las respuestas. Números más cercanos a cero serían más "objetivos" y números más cercanos a dos serían más "creativos".
-   **g_seed:** También podemos setear una semilla para hacer los resultados más reproducibles aunque no podemos asegurarlo completamente.
-   Otros parámetros que para este caso recomendamos no cambiarlos, sin emabrgo, su información puede encontrarse en la documentación de la API de Gemini.

```{r}
# Parámetros configurables
g_model = "2.5-flash-lite"
g_temperature = 0.7
g_seed = 42

# Los siguientes recomendamos no cambiarlos para pruebas básicas
# maxOutputTokens = 8192
# topK = 40
# topP = 0.95
# timeout = 60
```

## Prompts

Cuando diseñamos prompts buscamos, por lo general, la respuesta a cierta pregunta o la resolución de algún problema. Además, la forma en la que diseñamos un prompt influye estructuralmente en la respuesta que podamos recibir de un modelo de LLM. En el presente taller haremos el ejercicio interactivo de intentar responder a la siguiente pregunta de investigación: *¿Cómo se configura la construcción social y mediática de la Inteligencia Artificial en el discurso noticioso actual en medios de prensa escritos de alto impacto en Chile?*

En estricto rigor, Gemini no permite que le pasemos instrucciones de sistema, petición de texto y estructuras de respuesta por separado. Nosotros en este caso los separamos en objetos distintos solo para hacerlo más modular. Tenemos en este caso 3 partes, para comenzar a moldear las respuestas dependiendo de nuestro caso específico de estudio:

1.  **system_prompt:** La idea es darle una idea específica de como "qué tipo de persona" tiene que comportarse y qué campos de conocimientos debe manejar para responder (es distinto preguntarle de inteligencia artificial a un experto en ciencias de la computación o a un periodista). Podemos utilizar este recurso para delimitar ciertos marcos teóricos y disciplinares al momento de esperar una respuesta.

2.  **prompt_instruccion:** Como su nombre lo dice, las instrucciones específicas. En nuestro caso, aquí es donde operacionalizamos la pregunta de investigación. Debemos indicarle explícitamente al modelo qué dimensiones teóricas debe buscar. Aquí traducimos conceptos abstractos (como la construcción social o el encuadre mediático) en directrices concretas: evaluar si la tecnología se presenta como amenaza u oportunidad (tecnofobia/tecnofilia), identificar qué actores tienen voz (autoridad experta vs. sociedad civil) y detectar los sesgos latentes. Básicamente, esta sección funciona como nuestro libro de códigos o pauta de análisis cualitativo, asegurando que el modelo no divague, sino que interrogue al texto periodístico con un propósito definido.

3.  **json_estructura:** Finalmente, definimos el formato de salida. Aunque el análisis sea cualitativo, necesitamos sistematizar la información para poder trabajarla a escala en R. Al exigir una respuesta en formato JSON estricto, forzamos al modelo a sintetizar su razonamiento complejo en una estructura de datos legible por la máquina. En otras palabras, transformar cientos de análisis de texto individuales en un dataframe. Gracias a esto, podremos posteriormente calcular frecuencias, correlacionar variables (por ej. *¿un medio es más tecnofílico que otro? ¿los medios de la capital son más tecnofílicos que los de regiones rurales?*) y visualizar los hallazgos de nuestra investigación de manera cuantitativa.

Nuestro propósito aquí es mostrar la estructura que usamos para que un LLM siga nuestras instrucciones como queremos, pero para cada caso de uso el tipo de prompt, los marcos teóricos, preguntas específicas y estructura de respuesta que pidamos será distinta.

```{r}
system_prompt <- "Eres un científico social experto en análisis cualitativo con formación interdisciplinaria en sociología, antropología, ciencias políticas y ciencias de la comunicación. Tienes amplia experiencia en investigación de medios, análisis de discurso, estudios culturales y semiótica social. Posees la capacidad de examinar contenido mediático desde múltiples perspectivas teóricas, identificando marcos narrativos e interpretativos, agendas mediáticas, construcciones simbólicas y procesos de significación. Mantienes una perspectiva académica rigurosa y pensamiento crítico interdisciplinario."

prompt_instruccion <- "Analiza la siguiente noticia relacionada con inteligencia artificial desde una perspectiva de ciencias sociales y comunicación. Tu análisis debe ser profundo y reflexivo, examinando tanto el contenido explícito como los elementos implícitos y simbólicos.

Requisitos para el análisis:

- Determina el rol específico que cumple la IA en la noticia: ¿es el tema central, un elemento secundario, o apenas se menciona?
- Evalúa la centralidad de la IA en la narrativa y su relevancia real para el contenido
- Identifica orientaciones tecnofóbicas o tecnofílicas en el tratamiento informativo
- Examina cómo se representa la IA: sus capacidades, limitaciones, riesgos o beneficios
- Analiza los marcos interpretativos y narrativos utilizados para contextualizar la tecnología
- Detecta sesgos, omisiones o énfasis particulares en la construcción del discurso sobre IA
- Considera las implicaciones sociales, económicas, políticas o éticas planteadas
- Identifica actores sociales mencionados y sus posiciones respecto a la IA
- Evalúa qué perspectivas están presentes o ausentes en el debate
- Genera keywords conceptuales que capturen la esencia sociológica del contenido
- Proporciona un resumen analítico que sintetice los hallazgos principales

Mantén un enfoque crítico y académico, fundamentando tus observaciones únicamente en el contenido analizado."

json_estructura <- 'Responde exclusivamente en formato JSON válido. Tu respuesta debe ser ÚNICAMENTE así, comenzando con "{" y terminando con "}". 

** NO AGREGUES BLOQUES DE CÓDIGO EN TU RESPUESTA (de esas que tienen este caracter `)**

Consideraciones:
  - Las keywords deben ser 5, ir en minúsculas y si se componen de más de una palabra llevan espacios entre ellas.
  - Deben ser 5 actores principales, si no existe alguno entrega el campo igualmente con NULL.
  - Si el rol de un actor no está relacionado con la IA indícalo explícitamente.
  - Label: Máximo 3 palabras. Si la IA no es central en la noticia, los labels no deben estar relacionados con la IA.

Estructura:

{
  "keywords": ["palabra1", "palabra2", "palabra3"],
  "orientacion_tecnologica": "tecnofobia|tecnofilia|neutralidad",
  "centralidad_ia": "alta|media|baja",
  "actores_principales": [
    {
      "nombre": "nombre_del_actor",
      "rol_respecto_ia": "descripción_breve_de_su_posición_o_relación_con_ia"
    }
  ],
  "analisis": "Análisis conciso en 2-3 oraciones que sintetice los hallazgos más relevantes del enfoque sociológico y comunicacional.",
  "label": "etiqueta_descriptiva_de_la_noticia"
})
'
```

## Función para crear prompt pasándole las noticias

Aquí concatenamos las partes del prompt que hicimos antes y le entregamos un *input* estructurado al LLM, pasándole el título y el contenido de la noticia. Al final imprimimos el resultado del prompt para una sola fila.

```{r}
# Crear prompt completo con contexto
crear_prompt_completo <- function(fila_noticia) {
  # Extraer información de la fila
  titulo <- fila_noticia$titulo
  contenido <- fila_noticia$contenido_limpio
  fecha <- fila_noticia$fecha
  
  # Crear contexto de la noticia
  contexto_noticia <- paste0(
    "# CONTEXTO DE LA NOTICIA:\n",
    "Título: ", titulo, "\n",
    "CONTENIDO:\n", contenido
  )
  
  # Combinar todos los elementos del prompt
  prompt_final <- paste(
    system_prompt,
    "\n\n",
    prompt_instruccion,
    "\n\n",
    contexto_noticia,
    "\n\n",
    json_estructura,
    sep = ""
  )
  
  return(prompt_final)
}

# Aquí ejecutamos
prompt_example <- crear_prompt_completo(noticias_muestra[5,])
example <- gemini(
  prompt_example,
  model = g_model,
  temperature = g_temperature,
  seed = g_seed
)

cat(example)
```

## Iteración guardando en una lista

Iteramos por cada una de las filas del dataframe para obtener nuestra respuesta estructurada. Guardamos esto en una lista manejando los errores que puedan surgir. A veces, trabajando con API, una fila va a tener un error y nos puede detener el trabajo subsiguiente. Para ello implementamos una lógica de reintentos.

```{r}
lista_respuestas <- list()

lista_respuestas <- vector("list", nrow(noticias_muestra)) # Creamos posiciones vacías en la lista para cada fila de nuestro dataframe

for (i in 1:nrow(noticias_muestra)){
  noticia <- noticias_muestra[i,]
  prompt <- crear_prompt_completo(noticia)
  
  analisis <- NULL

  # Lógica de reintentos
  for(intento in 1:3){ 
    analisis <- tryCatch(
      gemini(prompt, model = g_model, temperature = g_temperature, seed = g_seed),
      error = function(e) { message(paste("Error en fila", i, "intento", intento)); return(NULL) }
    )
    if(!is.null(analisis)) break 
    Sys.sleep(2)
  }  
  if(!is.null(analisis)) {
    lista_respuestas[[i]] <- analisis
  } else {
    lista_respuestas[[i]] <- NA 
    message(paste("Fila", i, "quedó vacía por error en API."))
  }
  
  Sys.sleep(2)
}
```

Revisamos el caso de la quinta fila, que debiese ser muy parecido al del ejemplo inicial. Una posible vía de validación es que en diferentes llamadas un mismo caso va a dar resultados similares, aunque al tratarse de LLM es imposible asegurar una total repoducibilidad.

```{r}
cat(lista_respuestas[[5]])
```

## Limpieza y aplanamiento

Aplicamos una función de limpieza a la lista para eliminar los elementos *markdown* que nos entregó el LLM en la respuesta. Como *output* tendremos una lista concatenada con los distintos elementos que le solicitamos y lista para usar en R.

```{r}
lista_proc <- lapply(lista_respuestas, function(texto_sucio) {
  texto_limpio <- str_trim(str_remove_all(texto_sucio, "^```json|```$"))
  objeto_r <- fromJSON(texto_limpio, flatten = TRUE)
  return(objeto_r)
})

print(lista_proc[[1]])
```

Ya tenemos nuestro objeto de listas concatenadas, pero también podemos crear un dataframe para operarlo de mejor manera en R. En este caso tenemos una estructura algo compleja, ya que tenemos una lista que dentro también tiene un dataframe (con los actores y su rol). Entonces, creamos un dataframe aplanando nuestra lista.

```{r}
aplanar_elemento <- function(elemento) {
  data.frame(
    # Vectores simples
    keywords = paste(elemento$keywords, collapse = "; "),
    orientacion_tecnologica = elemento$orientacion_tecnologica,
    centralidad_ia = elemento$centralidad_ia,
    label = elemento$label,
    analisis = elemento$analisis,
    
    # Información de actores (concatenamos todo)
    actores_nombres = paste(elemento$actores_principales$nombre, collapse = "; "),
    actores_roles = paste(elemento$actores_principales$rol_respecto_ia, collapse = "; "),
    
    stringsAsFactors = FALSE
  )
}

# Aplicar a todos los elementos
df_proc <- purrr::map_dfr(lista_proc, aplanar_elemento)
```

Finalmente podemos unificar en un dataframe para tener la información original y la de análisis en un mismo lugar

```{r}
datos_unificados <- bind_cols(noticias_muestra, df_proc)
```

Y podemos guardar los datos (●'◡'●)

```{r}
#| eval: false

write.csv(datos_unificados, "datos_analizados.csv")
# Aquí también podemos importar el csv
# datos_unificados <- read.csv("datos_analizados.csv")
```

## Análisis descriptivos

Por motivos pedagógicos, vamos a cargar una base de datos más grande preprocesada por gemini (n=249), entre 2023 y 2025.

```{r}
datos_descriptivos <- read.csv("datos_descriptivos.csv")
```

Observamos la frecuencia de los datos a lo largo del tiempo. 
```{r}
datamedios::grafico_notas_fecha(datos_descriptivos, titulo="Noticias a lo largo del tiempo en los datos", agrupar_por = "month", tema = "dark")

datamedios::grafico_comparacion_medios(datos_descriptivos, titulo = "Comparación medios a lo largo del tiempo", agrupar_por = "month", tema = "dark")
```

Tenemos más datos de bíobío que de otros medios debido a su frecuencia de publicación. El medio guioteca tiene muy pocas noticias, por lo que no se puede interpretar mucho

### Preprocesamiento
```{r}

df <- datos_descriptivos %>%
  mutate(
    fecha = as.Date(fecha), # Convertimos a fecha
    centralidad_ia = factor(centralidad_ia, levels = c("baja", "media", "alta")),
    orientacion_tecnologica = factor(orientacion_tecnologica, 
                                     levels = c("tecnofobia", "neutralidad", "tecnofilia"))
  )
colores_orientacion <- c("tecnofobia" = "#D73027",
                         "neutralidad" = "#E0E0E0",
                         "tecnofilia" = "#4575B4")
                      
colores_centralidad <- c("baja" = "#7008E7", 
                         "media" = "#2AA63E", 
                         "alta" = "#FE9A37")
```

### Frecuencia de orientación tecnológica
```{r}

grafico_1 <- df %>%
  count(orientacion_tecnologica) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = orientacion_tecnologica, y = n, fill = orientacion_tecnologica)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), vjust = -0.5) +
  scale_fill_manual(values = colores_orientacion) +
  labs(title = "Distribución de la Orientación Tecnológica",
       subtitle = "¿Cómo se evalúa la IA en las noticias?",
       x = NULL, y = "Cantidad de noticias") +
  theme_minimal()

grafico_1

```

La orientación más presente en los medios es *neutralidad*, seguida de tecnofilia. Existe una baja proporción de noticias con una concepción negativa de la IA, de solo 10%.

### Frecuencia de centralidad de la IA

```{r}
grafico_2 <- df %>%
  count(centralidad_ia) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = centralidad_ia, y = n, fill= centralidad_ia)) +
  geom_col() + 
  scale_fill_manual(values = colores_centralidad) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), vjust = -0.5) +
  labs(title = "Centralidad de la IA en el discurso",
       subtitle = "Nivel de protagonismo de la tecnología en la noticia",
       x = "Nivel de Centralidad", y = "Cantidad de noticias") +
  theme_minimal()

grafico_2
```

La centralidad de la IA se reparte equitativamente entre baja, media y alta, con una proporción algo mayor de una gran importancia de la IA en las noticias (40%).

### Orientación tecnológica a través del tiempo

```{r}
df_tiempo <- df %>%
  mutate(mes = floor_date(fecha, "month")) %>%
  count(mes, orientacion_tecnologica)

grafico_3 <- df_tiempo %>%
  ggplot(aes(x = mes, y = n, color = orientacion_tecnologica)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = colores_orientacion) +
  labs(title = "Evolución de la Orientación Tecnológica",
       subtitle = "Frecuencia mensual de noticias",
       x = "Fecha", y = "Número de artículos", color = "Orientación") +
  theme_minimal()

grafico_3
```

Las tendencias se mantienen en el tiempo, con la neutralidad consistentemente siendo la categoía más alta, y la tecnofilia y la tecnofobia presentando picos en ciertos momentos. 

### Centralidad IA a través del tiempo

```{r}

df_tiempo_centralidad <- df %>%
  mutate(anio = as.factor(year(as.Date(fecha)))) %>%
  count(anio, centralidad_ia) %>%
  group_by(anio) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

grafico_4 <- df_tiempo_centralidad %>%
  ggplot(aes(x = anio, y = prop, fill = centralidad_ia)) +
  geom_col(position = "stack", width = 0.7) +
  geom_text(aes(label = scales::percent(prop, accuracy = 1)), 
            position = position_stack(vjust = 0.5), # Centrado en la barra
            color = "white", 
            fontface = "bold",
            size = 4) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_centralidad) +
  labs(title = "Evolución de la Centralidad de la IA por Año",
       subtitle = "Proporción de noticias según nivel de protagonismo",
       x = "Año",
       y = "Porcentaje",
       fill = "Centralidad") +
  theme_minimal() +
  theme(
    legend.position = "top",
    panel.grid.major.x = element_blank()
  )

grafico_4


```

De la misma forma, la centralidad de la IA se mantiene relativamente constante a lo largo del tiempo, aunque en 2023 hubo una mayor cantidad de noticias con un protagonismo alto de esta.

### Orientación tecnológica por medio
```{r}
grafico_5 <- df %>%
  count(medio, orientacion_tecnologica) %>%
  group_by(medio) %>%
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = medio, y = pct, fill = orientacion_tecnologica)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_orientacion) +
  coord_flip() +
  labs(title = "Orientación Tecnológica por Medio",
       subtitle = "Proporción de noticias positivas, neutras y negativas",
       x = NULL, y = "Porcentaje", fill = "Orientación") +
  theme_minimal()

grafico_5
```

Se aprecian diferencias en la narrativa en torno a la IA presente en los medios: emol y medios regionales presentan un discurso tecnofílico más marcado, mientras que en bíobío predomina de forma más evidente la neutralidad. 

### Centralidad IA por medio

```{r}
grafico_6 <- df %>%
  count(medio, centralidad_ia) %>%
  group_by(medio) %>%
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = medio, y = pct, fill = centralidad_ia)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = colores_centralidad) +
  coord_flip() +
  labs(title = "Centralidad IA por Medio",
       subtitle = "Proporción de noticias positivas, neutras y negativas",
       x = NULL, y = "Porcentaje", fill = "Orientación") +
  theme_minimal()

grafico_6
```

Con respecto a la centralidad de la IA, también se aprecian diferencias entre los medios. El diario Emol, consistentemente con el análisis previo, tiende a resaltar la relevancia de la IA en sus noticias, mientras que en BioBio la centralidad baja es más predominante. 

### Nube de palabras columna análisis

```{r}
# Stop words

lista_base <- tibble(word = tm::stopwords("es"))

mas_stop_words <- tibble(word = c(
  "inteligencia", "artificial", "noticia", "noticias",
  "orientación", "tecnológica", "ia", "dentro", "destacando",
  "enmarca", "centra", "clara", "discurso", "si", "énfasis",
  "forma", "presenta", "narrativa", "través", "figura", "factor",
  "sino", "busca", "chile", "abordar", "chileno", "sector",
  "actores", "apoyo", "oscila", "enfatizan", "generar",
  "ser", "allá", "análisis", "actúa", "hacia", "tecnología",
  "observa", "elemento", "foco", "cómo", "marco", "aborda", "utiliza", "posibles", "enmarcando", "utilizando", "construcción", "discursiva", "marcos", "narrativos",
  "principal", "recae", "presentando", "informativo", "tratamiento", "agenda", "mediática",
  "eje", "central", "tema", "predominantemente", "asociados", "omitiendo", "manera", "mencionada"
))

stop_words_final <- bind_rows(lista_base, mas_stop_words)

palabras_para_nube <- df %>%
  # Separar en palabras individuales
  unnest_tokens(word, analisis) %>%
  # Quitar espacios en blanco extra
  mutate(word = str_trim(word)) %>%
  # Eliminar palabras que estén en nuestra lista de stop words
  anti_join(stop_words_final, by = "word") %>%
  # Contar frecuencia
  count(word, sort = TRUE)

wordcloud2(palabras_para_nube, size = 0.7, color = "random-dark")

```

En el análisis realizado por Gemini, existe una prevalencia de ciertas palabras por sobre otras. La palabra tecnofílica es la más mencionada, sustentada en discursos relacionados con la "eficciencia", el "avance", los "beneficios" y el "desarrollo". Por otra parte, se destacan narrativas con un tono más crítico, con una alta frecuencia de palabras como "riesgos", "implicaciones" y "éticas".

### Bigramas análisis

```{r}
# Tokenizar en pares de palabras (bigramas)
bigramas_analisis <- df %>%
  unnest_tokens(bigram, analisis, token = "ngrams", n = 2) %>%
  # Separar para filtrar stopwords
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words_final$word,
         !word2 %in% stop_words_final$word) %>% 
  # Unir de nuevo
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

# Graficar los Top 15 bigramas
grafico_6 <- bigramas_analisis %>%
  head(20) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = bigram, y = n)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(title = "Frases más comunes en el análisis cualitativo",
       subtitle = "Bigramas más frecuentes",
       x = NULL, y = "Frecuencia") +
  theme_minimal()

print(grafico_6)
```

Observando el análisis de los bigramas, se obtiene una mirada más profunda del análisis realizado por el LLM. "Debates éticos", "implicaciones sociales" y "riesgos inherentes" son de los conceptos más frecuentemente utilizados en conjunto, conectados con la narrativa más alarmista en torno a los avances de la IA. Por otro lado, se encuentran conceptos como "avance tecnológico", "herramienta innovadora" "herramienta democratizadora" y "desarrollo económico", asociados al discurso tecnofílico. Encontrando otros marcos interpretativos que presentan las noticias, se observan conceptos como "competencia geopolítica", "propiedad intelectual", "mercado laboral" e incluso "ciencia ficción", siendo estos temas centrales en la discusión contemporánea sobre la IA.

## Conclusión

- Revisamos los avances recientes de la IA y sus aplicaciones al análisis de texto
- Aprendimos un flujo de trabajo automatizado para realizar análisis cualitativo en grandes volúmenes de texto, aplicable para múltiples disciplinas y a través de diferentes modelos
- Profundizamos en distintas prácticas para el correcto funcionamiento de estas tecnologías, como el uso de prompts precisos y estructurados
- Procesamos estos datos para poder analizarlos en R, y observamos diferentes alternativas básicas para visualizarlos e interpretarlos

## Discusión 

- Si bien aún queda mucho camino por recorrer, aprender a utilizar este tipo de herramientas presenta grandes ventajas para la investigación cualitativa
- En la discusión reciente, se han destacado desafíos para implementar estas herramientas. 
- Entre los más relevantes, se encuentra el problema de la validación de los códigos generados, los posibles sesgos algorítmicos y de entrenamiento y la utilización de datos para entrenamiento.
- Para enfrentar estos problemas, es necesario estar informado, experimentar y continuar indagando sobre las potencialidades de estas herramientas. Sin duda alguna, el futuro del análisis cualitativo tendrá una mucho mayor presencia de LLMs, ya que reducen considerablemente la carga de trabajo y abren posibilidades de análisis antes inimaginables, y se encuentran en un acelerado desarrollo.
- El uso correcto de estas tecnologías debe ser con una colaboración fuerte entre humano y máquina, y no algo meramente automático. Es crucial que aprendamos a utilizarlas en entornos que nos permitan tomar las decisiones importantes, y no desde las plataformas de consumo, en donde estas opciones vendrán por defecto.